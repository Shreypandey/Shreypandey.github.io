---
---


@INPROCEEDINGS{9565973,
  author={Pandey, Shrey and Srivastava, Yash and Meena, Yukta and Dewang, Rupesh Kumar},
  booktitle={2021 8th International Conference on Signal Processing and Integrated Networks (SPIN)}, 
  title={CLOTON: A GAN based approach for Clothing Try-On}, 
  year={2021},
  volume={},
  number={},
  pages={595-601},
  abstract={Cloton is a framework that produces a picture of a person wearing the target clothing while taking into account arbitrary body shapes and poses. As the generation of an individual image necessitates the extraction of body shape and pose from one image and target clothing from another, the architecture is divided into a number of tasks. With these two images as input, the system generates a new picture of the user wearing the target clothing using a generative network. The output is this generated image, which is then sent back to the user. By using encoder-decoder networks as the generator model and training it using the generative adversarial approach used by GANs, Cloton manages to generate visually appealing garment swapped images of the user. Cloton also has good results in the quantitative analysis of the system as compared to previous works.},
  keywords={Training;Shape;Statistical analysis;Clothing;Fitting;Signal processing;Generative adversarial networks},
  doi={10.1109/SPIN52536.2021.9565973},
  ISSN={2688-769X},
  month={Aug},}

@inproceedings{10.1145/3564121.3564799,
author = {Patel, Dhruv and Pandey, Shrey and Sharma, Abhishek},
title = {Efficient Vector Store System for Python using Shared Memory},
year = {2023},
isbn = {9781450398473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564121.3564799},
doi = {10.1145/3564121.3564799},
abstract = {Many e-commerce companies use machine learning to make customer experience better. Even within a single company, there will be generally many independent services running, each specializing in some aspect of customer experience. Since machine learning models work on abstract vectors representing users and/or items, each such service needs a way to store these vectors. A common approach nowadays is to save them in in-memory caches like Memcached. As these caches run in their own processes, and Machine Learning services generally run as Python services, there is a communication overhead involved for each request that ML service serves. One can reduce this overhead by directly storing these vectors in a Python dictionary within the service. To support concurrency and scale, a single node runs multiple instances of the same service. Thus, we also want to avoid duplicating these vectors across multiple processes. In this paper, we propose a system to store vectors in shared memory and efficiently serve all concurrent instances of the service, without replicating the vectors themselves. We achieve up to 4.5x improvements in latency compared to Memcached. Additionally, due to availability of more memory, we can increase the number of server processes running in each node, translating into greater throughput. We also discuss the impact of the proposed method (towards increasing the throughput) in live production scenario.},
booktitle = {Proceedings of the Second International Conference on AI-ML Systems},
articleno = {22},
numpages = {6},
keywords = {vector data store, shared memory, embedding data store},
location = {Bangalore, India},
series = {AIMLSystems '22}
}

